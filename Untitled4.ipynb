{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNJQxxJonsskCuCJjHjIGh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohanasamanya/MachineLearning/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UniEKk2LED_D",
        "outputId": "c6a9b95b-7644-4f28-ebf8-471ee9e8e621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regression Coefficients (Full-Batch Gradient Descent):\n",
            "Intercept: 1.2328099487610318\n",
            "Slope: 1.170263693076768\n",
            "Sum Squared Error (SSE): 5.624278989977716\n",
            "R-squared Value: 0.9472641444915357\n",
            "for analytical: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.2363636363636368, 1.1696969696969697, 5.624242424242421, 0.952538038613988)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Data\n",
        "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])\n",
        "\n",
        "# Number of data points\n",
        "n = len(x)\n",
        "\n",
        "# Sums\n",
        "sum_x = np.sum(x)\n",
        "sum_y = np.sum(y)\n",
        "sum_xx = np.sum(x**2)\n",
        "sum_xy = np.sum(x*y)\n",
        "\n",
        "# Coefficients\n",
        "beta_1 = (n * sum_xy - sum_x * sum_y) / (n * sum_xx - sum_x**2)\n",
        "beta_0 = (sum_y - beta_1 * sum_x) / n\n",
        "\n",
        "# Predicted y\n",
        "y_pred = beta_0 + beta_1 * x\n",
        "\n",
        "# Sum Squared Error (SSE)\n",
        "SSE = np.sum((y - y_pred)**2)\n",
        "\n",
        "# Total Sum of Squares (SST)\n",
        "SST = np.sum((y - np.mean(y))**2)\n",
        "\n",
        "# R-squared value\n",
        "R2 = 1 - (SSE / SST)\n",
        "\n",
        "beta_0, beta_1, SSE, R2\n",
        "\n",
        "\n",
        "#Gradient descendent method\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Data\n",
        "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])\n",
        "\n",
        "# Add a column of ones to x for the bias term\n",
        "x = np.column_stack((np.ones(len(x)), x))\n",
        "\n",
        "# Initialize the regression coefficients\n",
        "theta = np.zeros(2)\n",
        "\n",
        "# Set the learning rate and number of iterations\n",
        "alpha = 0.01\n",
        "num_iterations = 1000\n",
        "\n",
        "# Perform full-batch gradient descent\n",
        "for _ in range(num_iterations):\n",
        "    # Compute the predicted values\n",
        "    y_pred_grad = x.dot(theta)\n",
        "\n",
        "    # Compute the error\n",
        "    error = y_pred_grad - y\n",
        "\n",
        "    # Compute the gradient\n",
        "    gradient = (2 / len(x)) * x.T.dot(error)\n",
        "\n",
        "    # Update the regression coefficients\n",
        "    theta -= alpha * gradient\n",
        "\n",
        "# Extract the regression coefficients\n",
        "beta_0_gd = theta[0]\n",
        "beta_1_gd = theta[1]\n",
        "\n",
        "# Print the regression coefficients\n",
        "print(\"Regression Coefficients (Full-Batch Gradient Descent):\")\n",
        "print(\"Intercept:\", beta_0_gd)\n",
        "print(\"Slope:\", beta_1_gd)\n",
        "\n",
        "# Compute the predicted values\n",
        "y_pred_grad = x.dot(theta)\n",
        "\n",
        "# Compute the Sum Squared Error (SSE)\n",
        "SSE_grad = np.sum((y - y_pred_grad) ** 2)\n",
        "print(\"Sum Squared Error (SSE):\", SSE_grad)\n",
        "\n",
        "# Compute the R-squared value\n",
        "R2_grad = 1 - (SSE_grad / ((len(y) - 1) * np.var(y)))\n",
        "print(\"R-squared Value:\", R2_grad)\n",
        "print(\"For analytical: \")\n",
        "beta_0, beta_1, SSE, R2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load the Boston Housing Rate Dataset\n",
        "boston = load_boston()\n",
        " data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "    target = raw_df.values[1::2, 2]\n",
        "# Select the RM attribute as the input feature\n",
        "X = boston.data[:, 5].reshape(-1, 1)\n",
        "y = boston.target\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model to the data\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print the regression coefficients\n",
        "print(\"Regression Coefficients (Analytic Formulation):\")\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "print(\"Slope:\", model.coef_)\n",
        "\n",
        "# Compute the predicted values\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Compute the Sum Squared Error (SSE)\n",
        "SSE = np.sum((y - y_pred) ** 2)\n",
        "print(\"Sum Squared Error (SSE):\", SSE)\n",
        "\n",
        "# Compute the R-squared value\n",
        "R2 = model.score(X, y)\n",
        "print(\"R-squared Value:\", R2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "w_3b4Dw4PATK",
        "outputId": "fc193e82-1e21-49c5-a383-dade8114e551"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-6-1875d26cf2eb>, line 7)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-1875d26cf2eb>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import pandas as pd # import pandas for data manipulation\n",
        "\n",
        "\n",
        "# Load the Boston Housing Rate Dataset\n",
        "#boston = load_boston() # This function is deprecated\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\" # remove the indent\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "target = raw_df.values[1::2, 2]\n",
        "# Select the RM attribute as the input feature\n",
        "X = data[:, 5].reshape(-1, 1) # use data instead of boston.data\n",
        "y = target # use target instead of boston.target\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model to the data\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print the regression coefficients\n",
        "print(\"Regression Coefficients (Analytic Formulation):\")\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "print(\"Slope:\", model.coef_)\n",
        "\n",
        "# Compute the predicted values\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Compute the Sum Squared Error (SSE)\n",
        "SSE = np.sum((y - y_pred) ** 2)\n",
        "print(\"Sum Squared Error (SSE):\", SSE)\n",
        "\n",
        "# Compute the R-squared value\n",
        "R2 = model.score(X, y)\n",
        "print(\"R-squared Value:\", R2)\n",
        "\n",
        "#gradient descendent\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Load the Boston Housing Rate Dataset\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "target = raw_df.values[1::2, 2]\n",
        "\n",
        "# Select the RM attribute as the input feature\n",
        "X = data[:, 5].reshape(-1, 1)\n",
        "y = target\n",
        "\n",
        "# Add a column of ones to X for the bias term\n",
        "X = np.column_stack((np.ones(len(X)), X))\n",
        "\n",
        "# Initialize the regression coefficients\n",
        "theta = np.zeros(2)\n",
        "\n",
        "# Set the learning rate and number of iterations\n",
        "alpha = 0.01\n",
        "num_iterations = 1000\n",
        "\n",
        "\n",
        "\n",
        "# Perform full-batch gradient descent\n",
        "for _ in range(num_iterations):\n",
        "    # Compute the predicted values\n",
        "    y_pred = X.dot(theta)\n",
        "\n",
        "    # Compute the error\n",
        "    error = y_pred - y\n",
        "\n",
        "    # Compute the gradient\n",
        "    gradient = (2 / len(X)) * X.T.dot(error)\n",
        "\n",
        "    # Update the regression coefficients\n",
        "    theta -= alpha * gradient\n",
        "\n",
        "# Print the regression coefficients\n",
        "print(\"Regression Coefficients (Full-Batch Gradient Descent):\")\n",
        "print(\"Intercept:\", theta[0])\n",
        "print(\"Slope:\", theta[1])\n",
        "\n",
        "# Compute the predicted values\n",
        "y_pred = X.dot(theta)\n",
        "\n",
        "# Compute the Sum Squared Error (SSE)\n",
        "SSE = np.sum((y - y_pred) ** 2)\n",
        "print(\"Sum Squared Error (SSE):\", SSE)\n",
        "\n",
        "# Compute the R-squared value\n",
        "R2 = 1 - (SSE / ((len(y) - 1) * np.var(y)))\n",
        "print(\"R-squared Value:\", R2)\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the regression coefficients\n",
        "theta = np.zeros(2)\n",
        "\n",
        "# Set the learning rate and number of iterations\n",
        "alpha = 0.01\n",
        "num_iterations = 1000\n",
        "\n",
        "# Perform stochastic gradient descent\n",
        "for _ in range(num_iterations):\n",
        "    for i in range(len(X)):\n",
        "        # Compute the predicted value for the current sample\n",
        "        y_pred = X[i].dot(theta)\n",
        "\n",
        "        # Compute the error for the current sample\n",
        "        error = y_pred - y[i]\n",
        "\n",
        "        # Compute the gradient for the current sample\n",
        "        gradient = 2 * X[i] * error\n",
        "\n",
        "        # Update the regression coefficients\n",
        "        theta -= alpha * gradient\n",
        "\n",
        "# Print the regression coefficients\n",
        "print(\"Regression Coefficients (Stochastic Gradient Descent):\")\n",
        "print(\"Intercept:\", theta[0])\n",
        "print(\"Slope:\", theta[1])\n",
        "\n",
        "# Compute the predicted values\n",
        "y_pred = X.dot(theta)\n",
        "\n",
        "# Compute the Sum Squared Error (SSE)\n",
        "SSE = np.sum((y - y_pred) ** 2)\n",
        "print(\"Sum Squared Error (SSE):\", SSE)\n",
        "\n",
        "# Compute the R-squared value\n",
        "R2 = 1 - (SSE / ((len(y) - 1) * np.var(y)))\n",
        "print(\"R-squared Value:\", R2)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvBw_GTJRtc1",
        "outputId": "313ac363-692b-483d-e8df-0c391bb03c19"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regression Coefficients (Analytic Formulation):\n",
            "Intercept: -34.67062077643857\n",
            "Slope: [9.10210898]\n",
            "Sum Squared Error (SSE): 22061.879196211798\n",
            "R-squared Value: 0.48352545599133423\n",
            "Regression Coefficients (Full-Batch Gradient Descent):\n",
            "Intercept: -6.970943890333078\n",
            "Slope: 4.747579345988098\n",
            "Sum Squared Error (SSE): 26845.286348783786\n",
            "R-squared Value: 0.3703001379142369\n",
            "Regression Coefficients (Stochastic Gradient Descent):\n",
            "Intercept: -12.007290075470893\n",
            "Slope: 4.230467524569683\n",
            "Sum Squared Error (SSE): 59984.42950939596\n",
            "R-squared Value: -0.40703237427270245\n"
          ]
        }
      ]
    }
  ]
}